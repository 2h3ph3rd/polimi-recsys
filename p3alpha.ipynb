{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base path for csv files\n",
    "base_path = \"data\"\n",
    "interactions_df_path = base_path + \"/interactions_and_impressions.csv\"\n",
    "items_length_df_path = base_path + \"/data_ICM_length.csv\"\n",
    "items_type_df_path = base_path + \"/data_ICM_type.csv\"\n",
    "users_df_path = base_path + \"/data_target_users_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype={0:int, 1:int, 2:str, 3:int}\n",
    "interactions_df = pd.read_csv(filepath_or_buffer=interactions_df_path, dtype=dtype)\n",
    "items_length_pf = pd.read_csv(filepath_or_buffer=items_length_df_path)\n",
    "items_types_df = pd.read_csv(filepath_or_buffer=items_type_df_path)\n",
    "users_df = pd.read_csv(filepath_or_buffer=users_df_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    \"\"\"\n",
    "    This function takes a matrix as input and transforms it into the specified format.\n",
    "    The matrix in input can be either sparse or ndarray.\n",
    "    If the matrix in input has already the desired format, it is returned as-is\n",
    "    the dtype parameter is always applied and the default is np.float32\n",
    "    :param X:\n",
    "    :param format:\n",
    "    :param dtype:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "\n",
    "    elif format == 'npy':\n",
    "        if sps.issparse(X):\n",
    "            return X.toarray().astype(dtype)\n",
    "        else:\n",
    "            return np.array(X)\n",
    "\n",
    "    elif isinstance(X, np.ndarray):\n",
    "        X = sps.csr_matrix(X, dtype=dtype)\n",
    "        X.eliminate_zeros()\n",
    "        return check_matrix(X, format=format, dtype=dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarityMatrixTopK(item_weights, k=100, verbose = False, use_absolute_values = False):\n",
    "    \"\"\"\n",
    "    The function selects the TopK most similar elements, column-wise\n",
    "    :param item_weights:\n",
    "    :param forceSparseOutput:\n",
    "    :param k:\n",
    "    :param verbose:\n",
    "    :param inplace: Default True, WARNING matrix will be modified\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    assert (item_weights.shape[0] == item_weights.shape[1]), \"selectTopK: ItemWeights is not a square matrix\"\n",
    "\n",
    "    n_items = item_weights.shape[0]\n",
    "    similarity_builder = Incremental_Similarity_Builder(n_items, initial_data_block=n_items*k, dtype = np.float32)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Generating topK matrix\")\n",
    "\n",
    "    # for each column, keep only the top-k scored items\n",
    "    sparse_weights = not isinstance(item_weights, np.ndarray)\n",
    "\n",
    "    # iterate over each column and keep only the top-k similar items\n",
    "    if sparse_weights:\n",
    "        item_weights = check_matrix(item_weights, format='csc', dtype=np.float32)\n",
    "\n",
    "\n",
    "    for item_idx in range(n_items):\n",
    "\n",
    "        if sparse_weights:\n",
    "            start_position = item_weights.indptr[item_idx]\n",
    "            end_position = item_weights.indptr[item_idx+1]\n",
    "\n",
    "            column_data = item_weights.data[start_position:end_position]\n",
    "            column_row_index = item_weights.indices[start_position:end_position]\n",
    "\n",
    "        else:\n",
    "            column_data = item_weights[:,item_idx]\n",
    "            column_row_index = np.arange(n_items, dtype=np.int32)\n",
    "\n",
    "        if np.any(column_data==0):\n",
    "            non_zero_data = column_data!=0\n",
    "            column_data = column_data[non_zero_data]\n",
    "            column_row_index = column_row_index[non_zero_data]\n",
    "\n",
    "\n",
    "        # If there is less data than k, there is no need to sort\n",
    "        if k < len(column_data):\n",
    "            # Use argpartition because I only need to select \"which\" are the topK elements, I do not need their exact order\n",
    "            if use_absolute_values:\n",
    "                top_k_idx = np.argpartition(-np.abs(column_data), k-1, axis=0)[:k]\n",
    "            else:\n",
    "                top_k_idx = np.argpartition(-column_data, k-1, axis=0)[:k]\n",
    "\n",
    "            try:\n",
    "                column_row_index = column_row_index[top_k_idx]\n",
    "                column_data = column_data[top_k_idx]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        similarity_builder.add_data_lists(row_list_to_add = column_row_index,\n",
    "                                          col_list_to_add = np.ones(len(column_row_index), dtype = np.int) * item_idx,\n",
    "                                          data_list_to_add = column_data)\n",
    "\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "    return similarity_builder.get_SparseMatrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_to_biggest_unit(time_in_seconds):\n",
    "\n",
    "    conversion_factor_list = [\n",
    "        (\"sec\", 1),\n",
    "        (\"min\", 60),\n",
    "        (\"hour\", 60),\n",
    "        (\"day\", 24),\n",
    "        (\"year\", 365),\n",
    "    ]\n",
    "\n",
    "    unit_index = 0\n",
    "    temp_time_value = time_in_seconds\n",
    "    new_time_value = time_in_seconds\n",
    "    new_time_unit = \"sec\"\n",
    "\n",
    "    while temp_time_value >= 1.0 and unit_index < len(conversion_factor_list)-1:\n",
    "\n",
    "        temp_time_value = temp_time_value/conversion_factor_list[unit_index+1][1]\n",
    "\n",
    "        if temp_time_value >= 1.0:\n",
    "            unit_index += 1\n",
    "            new_time_value = temp_time_value\n",
    "            new_time_unit = conversion_factor_list[unit_index][0]\n",
    "\n",
    "    else:\n",
    "        return new_time_value, "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIO(object):\n",
    "    \"\"\" DataIO\"\"\"\n",
    "\n",
    "    _DEFAULT_TEMP_FOLDER = \".temp\"\n",
    "\n",
    "    # _MAX_PATH_LENGTH_LINUX = 4096\n",
    "    _MAX_PATH_LENGTH_WINDOWS = 255\n",
    "\n",
    "    def __init__(self, folder_path):\n",
    "        super(DataIO, self).__init__()\n",
    "\n",
    "        self._is_windows = platform.system() == \"Windows\"\n",
    "\n",
    "        self.folder_path = folder_path if folder_path[-1] == \"/\" else folder_path + \"/\"\n",
    "        self._key_string_alert_done = False\n",
    "\n",
    "        # if self._is_windows:\n",
    "        #     self.folder_path = \"\\\\\\\\?\\\\\" + self.folder_path\n",
    "\n",
    "\n",
    "    def _print(self, message):\n",
    "        print(\"{}: {}\".format(\"DataIO\", message))\n",
    "\n",
    "\n",
    "    def _get_temp_folder(self, file_name):\n",
    "        \"\"\"\n",
    "        Creates a temporary folder to be used during the data saving\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Ignore the .zip extension\n",
    "        file_name = file_name[:-4]\n",
    "        current_temp_folder = \"{}{}_{}_{}/\".format(self.folder_path, self._DEFAULT_TEMP_FOLDER, os.getpid(), file_name)\n",
    "\n",
    "        if os.path.exists(current_temp_folder):\n",
    "            self._print(\"Folder {} already exists, could be the result of a previous failed save attempt or multiple saver are active in parallel. \" \\\n",
    "            \"Folder will be removed.\".format(current_temp_folder))\n",
    "\n",
    "            shutil.rmtree(current_temp_folder, ignore_errors=True)\n",
    "\n",
    "        os.makedirs(current_temp_folder)\n",
    "\n",
    "        return current_temp_folder\n",
    "\n",
    "\n",
    "    def _check_dict_key_type(self, dict_to_save):\n",
    "        \"\"\"\n",
    "        Check whether the keys of the dictionary are string. If not, transforms them into strings\n",
    "        :param dict_to_save:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        all_keys_are_str = all(isinstance(key, str) for key in dict_to_save.keys())\n",
    "\n",
    "        if all_keys_are_str:\n",
    "            return dict_to_save\n",
    "\n",
    "        if not self._key_string_alert_done:\n",
    "            self._print(\"Json dumps supports only 'str' as dictionary keys. Transforming keys to string, note that this will alter the mapper content.\")\n",
    "            self._key_string_alert_done = True\n",
    "\n",
    "        dict_to_save_key_str = {str(key):val for (key,val) in dict_to_save.items()}\n",
    "\n",
    "        assert len(dict_to_save_key_str) == len(dict_to_save), \\\n",
    "            \"DataIO: Transforming dictionary keys into strings altered its content. Duplicate keys may have been produced.\"\n",
    "\n",
    "        return dict_to_save_key_str\n",
    "\n",
    "\n",
    "    def save_data(self, file_name, data_dict_to_save):\n",
    "\n",
    "        # If directory does not exist, create with .temp_model_folder\n",
    "        if not os.path.exists(self.folder_path):\n",
    "            os.makedirs(self.folder_path)\n",
    "\n",
    "        if file_name[-4:] != \".zip\":\n",
    "            file_name += \".zip\"\n",
    "\n",
    "\n",
    "        current_temp_folder = self._get_temp_folder(file_name)\n",
    "\n",
    "        try:\n",
    "\n",
    "            data_format = {}\n",
    "            attribute_to_save_as_json = {}\n",
    "\n",
    "            data_dict_to_save = self._check_dict_key_type(data_dict_to_save)\n",
    "\n",
    "            for attrib_name, attrib_data in data_dict_to_save.items():\n",
    "\n",
    "                current_file_path = current_temp_folder + attrib_name\n",
    "\n",
    "                if isinstance(attrib_data, DataFrame):\n",
    "                    # attrib_data.to_hdf(current_file_path + \".h5\", key=\"DataFrame\", mode='w', append = False, format=\"table\")\n",
    "                    # Save human readable version as a precaution. Append \".\" so that it is classified as auxiliary file and not loaded\n",
    "                    attrib_data.to_csv(current_temp_folder + \".\" + attrib_name + \".csv\", index=True)\n",
    "\n",
    "                    # Using \"fixed\" as a format causes a PerformanceWarning because it saves types that are not native of C\n",
    "                    # This is acceptable because it provides the flexibility of using python objects as types (strings, None, etc..)\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.filterwarnings(\"ignore\")\n",
    "                        attrib_data.to_hdf(current_file_path + \".h5\", key=\"DataFrame\", mode='w', append = False, format=\"fixed\")\n",
    "\n",
    "\n",
    "                elif isinstance(attrib_data, sps.spmatrix):\n",
    "                    sps.save_npz(current_file_path, attrib_data)\n",
    "\n",
    "                elif isinstance(attrib_data, np.ndarray):\n",
    "                    # allow_pickle is FALSE to prevent using pickle and ensure portability\n",
    "                    np.save(current_file_path, attrib_data, allow_pickle=False)\n",
    "\n",
    "                else:\n",
    "                    # Try to parse it as json, if it fails and the data is a dictionary, use another zip file\n",
    "                    try:\n",
    "                        _ = json.dumps(attrib_data, default=json_not_serializable_handler)\n",
    "                        attribute_to_save_as_json[attrib_name] = attrib_data\n",
    "\n",
    "                    except TypeError:\n",
    "\n",
    "                        if isinstance(attrib_data, dict):\n",
    "                            dataIO = DataIO(folder_path = current_temp_folder)\n",
    "                            dataIO.save_data(file_name = attrib_name, data_dict_to_save=attrib_data)\n",
    "\n",
    "                        else:\n",
    "                            raise TypeError(\"Type not recognized for attribute: {}\".format(attrib_name))\n",
    "\n",
    "\n",
    "\n",
    "            # Save list objects\n",
    "            if len(data_format)>0:\n",
    "                attribute_to_save_as_json[\".data_format\"] = data_format.copy()\n",
    "\n",
    "            for attrib_name, attrib_data in attribute_to_save_as_json.items():\n",
    "                current_file_path = current_temp_folder + attrib_name\n",
    "\n",
    "                # if self._is_windows and len(current_file_path + \".json\") >= self._MAX_PATH_LENGTH_WINDOWS:\n",
    "                #     current_file_path = \"\\\\\\\\?\\\\\" + current_file_path\n",
    "\n",
    "                absolute_path = current_file_path + \".json\" if current_file_path.startswith(os.getcwd()) else os.getcwd() + current_file_path + \".json\"\n",
    "\n",
    "                assert not self._is_windows or (self._is_windows and len(absolute_path) <= self._MAX_PATH_LENGTH_WINDOWS), \\\n",
    "                    \"DataIO: Path of file exceeds {} characters, which is the maximum allowed under standard paths for Windows.\".format(self._MAX_PATH_LENGTH_WINDOWS)\n",
    "\n",
    "\n",
    "                with open(current_file_path + \".json\", 'w') as outfile:\n",
    "                    if isinstance(attrib_data, dict):\n",
    "                        attrib_data = self._check_dict_key_type(attrib_data)\n",
    "\n",
    "                    json.dump(attrib_data, outfile, default=json_not_serializable_handler)\n",
    "\n",
    "\n",
    "\n",
    "            with zipfile.ZipFile(self.folder_path + file_name + \".temp\", 'w', compression=zipfile.ZIP_DEFLATED) as myzip:\n",
    "                for file_to_compress in os.listdir(current_temp_folder):\n",
    "                    myzip.write(current_temp_folder + file_to_compress, arcname = file_to_compress)\n",
    "\n",
    "            # Replace file only after the new archive has been successfully created\n",
    "            # Prevents accidental deletion of previous versions of the file if the current write fails\n",
    "            os.replace(self.folder_path + file_name + \".temp\", self.folder_path + file_name)\n",
    "\n",
    "        except Exception as exec:\n",
    "\n",
    "            shutil.rmtree(current_temp_folder, ignore_errors=True)\n",
    "            raise exec\n",
    "\n",
    "\n",
    "        shutil.rmtree(current_temp_folder, ignore_errors=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load_data(self, file_name):\n",
    "\n",
    "        if file_name[-4:] != \".zip\":\n",
    "            file_name += \".zip\"\n",
    "\n",
    "        dataFile = zipfile.ZipFile(self.folder_path + file_name)\n",
    "\n",
    "        dataFile.testzip()\n",
    "\n",
    "        current_temp_folder = self._get_temp_folder(file_name)\n",
    "\n",
    "        try:\n",
    "\n",
    "            try:\n",
    "                data_format = dataFile.extract(\".data_format.json\", path = current_temp_folder)\n",
    "                with open(data_format, \"r\") as json_file:\n",
    "                    data_format = json.load(json_file)\n",
    "            except KeyError:\n",
    "                data_format = {}\n",
    "\n",
    "\n",
    "            data_dict_loaded = {}\n",
    "\n",
    "            for file_name in dataFile.namelist():\n",
    "\n",
    "                # Discard auxiliary data structures\n",
    "                if file_name.startswith(\".\"):\n",
    "                    continue\n",
    "\n",
    "                decompressed_file_path = dataFile.extract(file_name, path = current_temp_folder)\n",
    "                file_extension = file_name.split(\".\")[-1]\n",
    "                attrib_name = file_name[:-len(file_extension)-1]\n",
    "\n",
    "                if file_extension == \"csv\":\n",
    "                    # Compatibility with previous version\n",
    "                    attrib_data = pd.read_csv(decompressed_file_path, index_col=False)\n",
    "\n",
    "                elif file_extension == \"h5\":\n",
    "                    attrib_data = pd.read_hdf(decompressed_file_path, key=None, mode='r')\n",
    "\n",
    "                elif file_extension == \"npz\":\n",
    "                    attrib_data = sps.load_npz(decompressed_file_path)\n",
    "\n",
    "                elif file_extension == \"npy\":\n",
    "                    # allow_pickle is FALSE to prevent using pickle and ensure portability\n",
    "                    attrib_data = np.load(decompressed_file_path, allow_pickle=False)\n",
    "\n",
    "                elif file_extension == \"zip\":\n",
    "                    dataIO = DataIO(folder_path = current_temp_folder)\n",
    "                    attrib_data = dataIO.load_data(file_name = file_name)\n",
    "\n",
    "                elif file_extension == \"json\":\n",
    "                    with open(decompressed_file_path, \"r\") as json_file:\n",
    "                        attrib_data = json.load(json_file)\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"Attribute type not recognized for: '{}' of class: '{}'\".format(decompressed_file_path, file_extension))\n",
    "\n",
    "                data_dict_loaded[attrib_name] = attrib_data\n",
    "\n",
    "\n",
    "        except Exception as exec:\n",
    "\n",
    "            shutil.rmtree(current_temp_folder, ignore_errors=True)\n",
    "            raise exec\n",
    "\n",
    "        shutil.rmtree(current_temp_folder, ignore_errors=True)\n",
    "\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRecommender(object):\n",
    "    \"\"\"Abstract BaseRecommender\"\"\"\n",
    "\n",
    "    RECOMMENDER_NAME = \"Recommender_Base_Class\"\n",
    "\n",
    "    def __init__(self, URM_train, verbose=True):\n",
    "\n",
    "        super(BaseRecommender, self).__init__()\n",
    "\n",
    "        self.URM_train = check_matrix(URM_train.copy(), 'csr', dtype=np.float32)\n",
    "        self.URM_train.eliminate_zeros()\n",
    "\n",
    "        self.n_users, self.n_items = self.URM_train.shape\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.filterTopPop = False\n",
    "        self.filterTopPop_ItemsID = np.array([], dtype=np.int)\n",
    "\n",
    "        self.items_to_ignore_flag = False\n",
    "        self.items_to_ignore_ID = np.array([], dtype=np.int)\n",
    "\n",
    "        self._cold_user_mask = np.ediff1d(self.URM_train.indptr) == 0\n",
    "\n",
    "        if self._cold_user_mask.any():\n",
    "            self._print(\"URM Detected {} ({:4.1f}%) users with no interactions.\".format(\n",
    "                self._cold_user_mask.sum(), self._cold_user_mask.sum()/self.n_users*100))\n",
    "\n",
    "\n",
    "        self._cold_item_mask = np.ediff1d(self.URM_train.tocsc().indptr) == 0\n",
    "\n",
    "        if self._cold_item_mask.any():\n",
    "            self._print(\"URM Detected {} ({:4.1f}%) items with no interactions.\".format(\n",
    "                self._cold_item_mask.sum(), self._cold_item_mask.sum()/self.n_items*100))\n",
    "\n",
    "\n",
    "    def _get_cold_user_mask(self):\n",
    "        return self._cold_user_mask\n",
    "\n",
    "    def _get_cold_item_mask(self):\n",
    "        return self._cold_item_mask\n",
    "\n",
    "\n",
    "    def _print(self, string):\n",
    "        if self.verbose:\n",
    "            print(\"{}: {}\".format(self.RECOMMENDER_NAME, string))\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def get_URM_train(self):\n",
    "        return self.URM_train.copy()\n",
    "\n",
    "    def set_URM_train(self, URM_train_new, **kwargs):\n",
    "\n",
    "        assert self.URM_train.shape == URM_train_new.shape, \"{}: set_URM_train old and new URM train have different shapes\".format(self.RECOMMENDER_NAME)\n",
    "\n",
    "        if len(kwargs)>0:\n",
    "            self._print(\"set_URM_train keyword arguments not supported for this recommender class. Received: {}\".format(kwargs))\n",
    "\n",
    "        self.URM_train = check_matrix(URM_train_new.copy(), 'csr', dtype=np.float32)\n",
    "        self.URM_train.eliminate_zeros()\n",
    "\n",
    "        self._cold_user_mask = np.ediff1d(self.URM_train.indptr) == 0\n",
    "\n",
    "        if self._cold_user_mask.any():\n",
    "            self._print(\"Detected {} ({:4.1f}%) users with no interactions.\".format(\n",
    "                self._cold_user_mask.sum(), self._cold_user_mask.sum()/len(self._cold_user_mask)*100))\n",
    "\n",
    "\n",
    "\n",
    "    def set_items_to_ignore(self, items_to_ignore):\n",
    "        self.items_to_ignore_flag = True\n",
    "        self.items_to_ignore_ID = np.array(items_to_ignore, dtype=np.int)\n",
    "\n",
    "    def reset_items_to_ignore(self):\n",
    "        self.items_to_ignore_flag = False\n",
    "        self.items_to_ignore_ID = np.array([], dtype=np.int)\n",
    "\n",
    "\n",
    "    #########################################################################################################\n",
    "    ##########                                                                                     ##########\n",
    "    ##########                     COMPUTE AND FILTER RECOMMENDATION LIST                          ##########\n",
    "    ##########                                                                                     ##########\n",
    "    #########################################################################################################\n",
    "\n",
    "\n",
    "    def _remove_TopPop_on_scores(self, scores_batch):\n",
    "        scores_batch[:, self.filterTopPop_ItemsID] = -np.inf\n",
    "        return scores_batch\n",
    "\n",
    "\n",
    "    def _remove_custom_items_on_scores(self, scores_batch):\n",
    "        scores_batch[:, self.items_to_ignore_ID] = -np.inf\n",
    "        return scores_batch\n",
    "\n",
    "\n",
    "    def _remove_seen_on_scores(self, user_id, scores):\n",
    "\n",
    "        assert self.URM_train.getformat() == \"csr\", \"Recommender_Base_Class: URM_train is not CSR, this will cause errors in filtering seen items\"\n",
    "\n",
    "        seen = self.URM_train.indices[self.URM_train.indptr[user_id]:self.URM_train.indptr[user_id + 1]]\n",
    "\n",
    "        scores[seen] = -np.inf\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _compute_item_score(self, user_id_array, items_to_compute = None):\n",
    "        \"\"\"\n",
    "        :param user_id_array:       array containing the user indices whose recommendations need to be computed\n",
    "        :param items_to_compute:    array containing the items whose scores are to be computed.\n",
    "                                        If None, all items are computed, otherwise discarded items will have as score -np.inf\n",
    "        :return:                    array (len(user_id_array), n_items) with the score.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"BaseRecommender: compute_item_score not assigned for current recommender, unable to compute prediction scores\")\n",
    "\n",
    "\n",
    "    def recommend(self, user_id_array, cutoff = None, remove_seen_flag=True, items_to_compute = None,\n",
    "                  remove_top_pop_flag = False, remove_custom_items_flag = False, return_scores = False):\n",
    "\n",
    "        # If is a scalar transform it in a 1-cell array\n",
    "        if np.isscalar(user_id_array):\n",
    "            user_id_array = np.atleast_1d(user_id_array)\n",
    "            single_user = True\n",
    "        else:\n",
    "            single_user = False\n",
    "\n",
    "        if cutoff is None:\n",
    "            cutoff = self.URM_train.shape[1] - 1\n",
    "\n",
    "        cutoff = min(cutoff, self.URM_train.shape[1] - 1)\n",
    "\n",
    "        # Compute the scores using the model-specific function\n",
    "        # Vectorize over all users in user_id_array\n",
    "        scores_batch = self._compute_item_score(user_id_array, items_to_compute=items_to_compute)\n",
    "\n",
    "\n",
    "        for user_index in range(len(user_id_array)):\n",
    "\n",
    "            user_id = user_id_array[user_index]\n",
    "\n",
    "            if remove_seen_flag:\n",
    "                scores_batch[user_index,:] = self._remove_seen_on_scores(user_id, scores_batch[user_index, :])\n",
    "\n",
    "\n",
    "        if remove_top_pop_flag:\n",
    "            scores_batch = self._remove_TopPop_on_scores(scores_batch)\n",
    "\n",
    "        if remove_custom_items_flag:\n",
    "            scores_batch = self._remove_custom_items_on_scores(scores_batch)\n",
    "\n",
    "        # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "        # - Partition the data to extract the set of relevant items\n",
    "        # - Sort only the relevant items\n",
    "        # - Get the original item index\n",
    "        # relevant_items_partition is block_size x cutoff\n",
    "        relevant_items_partition = np.argpartition(-scores_batch, cutoff-1, axis=1)[:,0:cutoff]\n",
    "\n",
    "        # Get original value and sort it\n",
    "        # [:, None] adds 1 dimension to the array, from (block_size,) to (block_size,1)\n",
    "        # This is done to correctly get scores_batch value as [row, relevant_items_partition[row,:]]\n",
    "        relevant_items_partition_original_value = scores_batch[np.arange(scores_batch.shape[0])[:, None], relevant_items_partition]\n",
    "        relevant_items_partition_sorting = np.argsort(-relevant_items_partition_original_value, axis=1)\n",
    "        ranking = relevant_items_partition[np.arange(relevant_items_partition.shape[0])[:, None], relevant_items_partition_sorting]\n",
    "\n",
    "        ranking_list = [None] * ranking.shape[0]\n",
    "\n",
    "        # Remove from the recommendation list any item that has a -inf score\n",
    "        # Since -inf is a flag to indicate an item to remove\n",
    "        for user_index in range(len(user_id_array)):\n",
    "            user_recommendation_list = ranking[user_index]\n",
    "            user_item_scores = scores_batch[user_index, user_recommendation_list]\n",
    "\n",
    "            not_inf_scores_mask = np.logical_not(np.isinf(user_item_scores))\n",
    "\n",
    "            user_recommendation_list = user_recommendation_list[not_inf_scores_mask]\n",
    "            ranking_list[user_index] = user_recommendation_list.tolist()\n",
    "\n",
    "\n",
    "\n",
    "        # Return single list for one user, instead of list of lists\n",
    "        if single_user:\n",
    "            ranking_list = ranking_list[0]\n",
    "\n",
    "\n",
    "        if return_scores:\n",
    "            return ranking_list, scores_batch\n",
    "\n",
    "        else:\n",
    "            return ranking_list\n",
    "\n",
    "\n",
    "\n",
    "    #########################################################################################################\n",
    "    ##########                                                                                     ##########\n",
    "    ##########                                LOAD AND SAVE                                        ##########\n",
    "    ##########                                                                                     ##########\n",
    "    #########################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    def save_model(self, folder_path, file_name = None):\n",
    "        raise NotImplementedError(\"BaseRecommender: save_model not implemented\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load_model(self, folder_path, file_name = None):\n",
    "\n",
    "        if file_name is None:\n",
    "            file_name = self.RECOMMENDER_NAME\n",
    "\n",
    "        self._print(\"Loading model from file '{}'\".format(folder_path + file_name))\n",
    "\n",
    "        dataIO = DataIO(folder_path=folder_path)\n",
    "        data_dict = dataIO.load_data(file_name=file_name)\n",
    "\n",
    "        for attrib_name in data_dict.keys():\n",
    "             self.__setattr__(attrib_name, data_dict[attrib_name])\n",
    "\n",
    "        self._print(\"Loading complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSimilarityMatrixRecommender(BaseRecommender):\n",
    "    \"\"\"\n",
    "    This class refers to a BaseRecommender KNN which uses a similarity matrix, it provides two function to compute item's score\n",
    "    bot for user-based and Item-based models as well as a function to save the W_matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, URM_train, verbose=True):\n",
    "        super(BaseSimilarityMatrixRecommender, self).__init__(URM_train, verbose = verbose)\n",
    "\n",
    "        self._URM_train_format_checked = False\n",
    "        self._W_sparse_format_checked = False\n",
    "\n",
    "\n",
    "\n",
    "    def _check_format(self):\n",
    "\n",
    "        if not self._URM_train_format_checked:\n",
    "\n",
    "            if self.URM_train.getformat() != \"csr\":\n",
    "                self._print(\"PERFORMANCE ALERT compute_item_score: {} is not {}, this will significantly slow down the computation.\".format(\"URM_train\", \"csr\"))\n",
    "\n",
    "            self._URM_train_format_checked = True\n",
    "\n",
    "        if not self._W_sparse_format_checked:\n",
    "\n",
    "            if self.W_sparse.getformat() != \"csr\":\n",
    "                self._print(\"PERFORMANCE ALERT compute_item_score: {} is not {}, this will significantly slow down the computation.\".format(\"W_sparse\", \"csr\"))\n",
    "\n",
    "            self._W_sparse_format_checked = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save_model(self, folder_path, file_name = None):\n",
    "\n",
    "        if file_name is None:\n",
    "            file_name = self.RECOMMENDER_NAME\n",
    "\n",
    "        self._print(\"Saving model in file '{}'\".format(folder_path + file_name))\n",
    "\n",
    "        data_dict_to_save = {\"W_sparse\": self.W_sparse}\n",
    "\n",
    "        dataIO = DataIO(folder_path=folder_path)\n",
    "        dataIO.save_data(file_name=file_name, data_dict_to_save = data_dict_to_save)\n",
    "\n",
    "        self._print(\"Saving complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseItemSimilarityMatrixRecommender(BaseSimilarityMatrixRecommender):\n",
    "\n",
    "    def _compute_item_score(self, user_id_array, items_to_compute=None):\n",
    "        \"\"\"\n",
    "        URM_train and W_sparse must have the same format, CSR\n",
    "        :param user_id_array:\n",
    "        :param items_to_compute:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self._check_format()\n",
    "\n",
    "        user_profile_array = self.URM_train[user_id_array]\n",
    "\n",
    "        if items_to_compute is not None:\n",
    "            item_scores = - np.ones((len(user_id_array), self.n_items), dtype=np.float32)*np.inf\n",
    "            item_scores_all = user_profile_array.dot(self.W_sparse).toarray()\n",
    "            item_scores[:, items_to_compute] = item_scores_all[:, items_to_compute]\n",
    "        else:\n",
    "            item_scores = user_profile_array.dot(self.W_sparse).toarray()\n",
    "\n",
    "        return item_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P3alphaRecommender(BaseItemSimilarityMatrixRecommender):\n",
    "    \"\"\" P3alpha recommender \"\"\"\n",
    "\n",
    "    RECOMMENDER_NAME = \"P3alphaRecommender\"\n",
    "\n",
    "    def __init__(self, URM_train, verbose = True):\n",
    "        super(P3alphaRecommender, self).__init__(URM_train, verbose = verbose)\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"P3alpha(alpha={}, min_rating={}, topk={}, implicit={}, normalize_similarity={})\".format(self.alpha,\n",
    "                                                                            self.min_rating, self.topK, self.implicit,\n",
    "                                                                            self.normalize_similarity)\n",
    "\n",
    "    def fit(self, topK=100, alpha=1., min_rating=0, implicit=False, normalize_similarity=False):\n",
    "\n",
    "        self.topK = topK\n",
    "        self.alpha = alpha\n",
    "        self.min_rating = min_rating\n",
    "        self.implicit = implicit\n",
    "        self.normalize_similarity = normalize_similarity\n",
    "\n",
    "\n",
    "        #\n",
    "        # if X.dtype != np.float32:\n",
    "        #     print(\"P3ALPHA fit: For memory usage reasons, we suggest to use np.float32 as dtype for the dataset\")\n",
    "\n",
    "        if self.min_rating > 0:\n",
    "            self.URM_train.data[self.URM_train.data < self.min_rating] = 0\n",
    "            self.URM_train.eliminate_zeros()\n",
    "            if self.implicit:\n",
    "                self.URM_train.data = np.ones(self.URM_train.data.size, dtype=np.float32)\n",
    "\n",
    "        #Pui is the row-normalized urm\n",
    "        Pui = normalize(self.URM_train, norm='l1', axis=1)\n",
    "\n",
    "        #Piu is the column-normalized, \"boolean\" urm transposed\n",
    "        X_bool = self.URM_train.transpose(copy=True)\n",
    "        X_bool.data = np.ones(X_bool.data.size, np.float32)\n",
    "        #ATTENTION: axis is still 1 because i transposed before the normalization\n",
    "        Piu = normalize(X_bool, norm='l1', axis=1)\n",
    "        del(X_bool)\n",
    "\n",
    "        # Alfa power\n",
    "        if self.alpha != 1.:\n",
    "            Pui = Pui.power(self.alpha)\n",
    "            Piu = Piu.power(self.alpha)\n",
    "\n",
    "        # Final matrix is computed as Pui * Piu * Pui\n",
    "        # Multiplication unpacked for memory usage reasons\n",
    "        block_dim = 200\n",
    "        d_t = Piu\n",
    "\n",
    "        similarity_builder = Incremental_Similarity_Builder(Pui.shape[1], initial_data_block=Pui.shape[1]*self.topK, dtype = np.float32)\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_time_printBatch = start_time\n",
    "\n",
    "        for current_block_start_row in range(0, Pui.shape[1], block_dim):\n",
    "\n",
    "            if current_block_start_row + block_dim > Pui.shape[1]:\n",
    "                block_dim = Pui.shape[1] - current_block_start_row\n",
    "\n",
    "            similarity_block = d_t[current_block_start_row:current_block_start_row + block_dim, :] * Pui\n",
    "            similarity_block = similarity_block.toarray()\n",
    "\n",
    "            for row_in_block in range(block_dim):\n",
    "                row_data = similarity_block[row_in_block, :]\n",
    "                row_data[current_block_start_row + row_in_block] = 0\n",
    "\n",
    "                relevant_items_partition = np.argpartition(-row_data, self.topK-1, axis=0)[:self.topK]\n",
    "                row_data = row_data[relevant_items_partition]\n",
    "\n",
    "                # Incrementally build sparse matrix, do not add zeros\n",
    "                if np.any(row_data == 0.0):\n",
    "                    non_zero_mask = row_data != 0.0\n",
    "                    relevant_items_partition = relevant_items_partition[non_zero_mask]\n",
    "                    row_data = row_data[non_zero_mask]\n",
    "\n",
    "                similarity_builder.add_data_lists(row_list_to_add=np.ones(len(row_data), dtype = np.int) * (current_block_start_row + row_in_block),\n",
    "                                                  col_list_to_add=relevant_items_partition,\n",
    "                                                  data_list_to_add=row_data)\n",
    "\n",
    "\n",
    "            if time.time() - start_time_printBatch > 300 or current_block_start_row + block_dim == Pui.shape[1]:\n",
    "                new_time_value, new_time_unit = seconds_to_biggest_unit(time.time() - start_time)\n",
    "\n",
    "                self._print(\"Similarity column {} ({:4.1f}%), {:.2f} column/sec. Elapsed time {:.2f} {}\".format(\n",
    "                     current_block_start_row + block_dim,\n",
    "                    100.0 * float( current_block_start_row + block_dim) / Pui.shape[1],\n",
    "                    float( current_block_start_row + block_dim) / (time.time() - start_time),\n",
    "                    new_time_value, new_time_unit))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_printBatch = time.time()\n",
    "\n",
    "\n",
    "        self.W_sparse = similarity_builder.get_SparseMatrix()\n",
    "\n",
    "\n",
    "        if self.normalize_similarity:\n",
    "            self.W_sparse = normalize(self.W_sparse, norm='l1', axis=1)\n",
    "\n",
    "\n",
    "        if self.topK != False:\n",
    "            self.W_sparse = similarityMatrixTopK(self.W_sparse, k=self.topK)\n",
    "\n",
    "        self.W_sparse = check_matrix(self.W_sparse, format='csr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
